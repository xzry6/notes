{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previous Chapter: Neural Network](4-ann.ipynb)\n",
    "<br>\n",
    "[Next Chapter: Recurrent Neural Network](6-rnn.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "Here is an exercise of building a *Convolutional Neural Network* using *Tensorflow*. \n",
    "\n",
    "See [here](https://github.com/xzry6/notes/blob/master/transwarp/cnn.md) if you are not familiar with *Convolutional Neural Network*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## header\n",
    "###### write your code here ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MNIST data\n",
    "mnist = ... ###### write your code here ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## parameters\n",
    "learning_rate = ... ###### write your code here ######\n",
    "training_iters = ... ###### write your code here ######\n",
    "batch_size = ... ###### write your code here ######\n",
    "display_step = ... ###### write your code here ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## placeholder inputs\n",
    "x = ... ###### write your code here ######\n",
    "y = ... ###### write your code here ######\n",
    "# dropout is a probability for randomly dropping a unit away, it should be a float 32 value\n",
    "dropout = ... ###### write your code here ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## weights and biases\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd': tf.Variable(tf.random_normal([7 * 7 * 64, 1024])),\n",
    "    'output': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd': tf.Variable(tf.random_normal([1024])),\n",
    "    'output': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph\n",
    "\n",
    "In a traditional *Convolutional Neural Network*, we have several *convolution layers* and *pool layers* following by a *fully-connected layer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Covolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convolutional layer\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1],\n",
    "                     padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pool Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max pool layer\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1],\n",
    "                          strides=[1, k, k, 1],\n",
    "                          padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # reshape\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # convolution layer 1\n",
    "    conv1 = ... ###### write your code here ######\n",
    "    # max pooling layer 1\n",
    "    conv1 = ... ###### write your code here ######\n",
    "\n",
    "    # convolution layer 2\n",
    "    conv2 = ... ###### write your code here ######\n",
    "    # max pooling layer 2\n",
    "    conv2 = ... ###### write your code here ######\n",
    "\n",
    "    # fully connected layer\n",
    "    # reshape conv2 output to fit fully connected layer input\n",
    "    fc = tf.reshape(conv2, [-1, weights['wd'].get_shape().as_list()[0]])\n",
    "    # apply `tf.nn.relu()` on linear combination of `fc * w[wd] + b[wd]`\n",
    "    fc = ... ###### write your code here ######\n",
    "    \n",
    "    # apply dropout on fully connected layer\n",
    "    fc = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # output is also a linear combination\n",
    "    output = ... ###### write your code here ######\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicted value\n",
    "pred = conv_net(x, weights, biases, dropout)\n",
    "\n",
    "# cost and optimizer\n",
    "cost = ... ###### write your code here ######\n",
    "optimizer = ... ###### write your code here ######\n",
    "training_steps = ... ###### write your code here ######\n",
    "\n",
    "# accuracy\n",
    "correct_prediction = ... ###### write your code here ######\n",
    "accuracy = ... ###### write your code here ######\n",
    "\n",
    "# initialization\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "\n",
    "    # keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # run training steps, use `x = batch_x`, `y = batch_y` and `dropout = 0.5` here\n",
    "        sess.run(... ###### write your code here ######)\n",
    "\n",
    "        if step % display_step == 0:\n",
    "            # run batch loss and accuracy, use `x = batch_x`, `y = batch_y` and `dropout = 1` here\n",
    "            loss, acc = ... ###### write your code here ######\n",
    "\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256],\n",
    "                                      dropout: 1.}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previous Chapter: Neural Network](4-ann.ipynb)\n",
    "<br>\n",
    "[Next Chapter: Recurrent Neural Network](6-rnn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
