{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previous Chapter: Recurrent Neural Network](7-rnn.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Mode\n",
    "\n",
    "Here is a sample of running a *Neural Network* on distributed clustering. Because of the busted Tensorflow distributed management, it could not run now here. Hope it could still help you though.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Here is a mnist training example using a single-layer \n",
    "neural network and a softmax classifier.\n",
    "To run this file,\n",
    "1. please check '172.16.3.227:~/tensorflow/scripts'\n",
    "   and execute 'exec_mnist_distributed.sh'.\n",
    "2. execute 'python mnist_distributed.py \\\n",
    "            --job_name=worker \\\n",
    "            --task_index=${0|1|2}' on each worker\n",
    "   and 'python mnist_distributed.py \\\n",
    "        --job_name=ps \\\n",
    "        --task_index=0' on parameter server\n",
    "\n",
    "Check (https://www.tensorflow.org/versions/r0.10/how_tos/style_guide.html) for tensorflow styling guide.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# input or default parameters\n",
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"data_dir\",\n",
    "    \"/root/tensorflow/MNIST_data\",\n",
    "    \"Directory for storing mnist data\")\n",
    "flags.DEFINE_string(\n",
    "    \"log_dir\",\n",
    "    \"/root/tensorflow/logs/mnist_log\",\n",
    "    \"Directory for storing log\")\n",
    "flags.DEFINE_boolean(\n",
    "    \"download_only\",\n",
    "    False,\n",
    "    \"Only perform downloading of data\")\n",
    "flags.DEFINE_string(\n",
    "    \"job_name\",\n",
    "    None,\n",
    "    \"job name: worker or ps\")\n",
    "flags.DEFINE_integer(\n",
    "    \"task_index\",\n",
    "    None,\n",
    "    \"Worker task index, should be >= 0.\")\n",
    "flags.DEFINE_integer(\n",
    "    \"hidden_units\",\n",
    "    100,\n",
    "    \"Number of units in the hidden layer of the NN\")\n",
    "flags.DEFINE_integer(\n",
    "    \"training_steps\",\n",
    "    20000,\n",
    "    \"Number of (global) training steps to perform\")\n",
    "flags.DEFINE_integer(\n",
    "    \"batch_size\",\n",
    "    100,\n",
    "    \"Training batch size to be fetched each time\")\n",
    "flags.DEFINE_float(\n",
    "    \"learning_rate\",\n",
    "    0.01,\n",
    "    \"Learning rate in machine learning\")\n",
    "flags.DEFINE_string(\n",
    "    \"ps_hosts\",\n",
    "    \"172.16.3.230:2222\",\n",
    "    \"Comma-separated list of hostname:port pairs\")\n",
    "flags.DEFINE_string(\n",
    "    \"worker_hosts\",\n",
    "    \"172.16.3.227:2222,172.16.3.228:2222,172.16.3.229:2222\",\n",
    "    \"Comma-separated list of hostname:port pairs\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "IMAGE_PIXELS = 28\n",
    "\n",
    "\n",
    "def main(_):\n",
    "\n",
    "  # validate and print necessary arguments\n",
    "  if FLAGS.job_name is None or FLAGS.job_name == \"\":\n",
    "    raise ValueError(\"Must specify an explicit `job_name`\")\n",
    "  if FLAGS.task_index is None or FLAGS.task_index ==\"\":\n",
    "    raise ValueError(\"Must specify an explicit `task_index`\")\n",
    "  print(\"job name = %s\" % FLAGS.job_name)\n",
    "  print(\"task index = %d\" % FLAGS.task_index)\n",
    "\n",
    "  # parse the ps(es) and worker(s)\n",
    "  ps_spec = FLAGS.ps_hosts.split(\",\")\n",
    "  worker_spec = FLAGS.worker_hosts.split(\",\")\n",
    "  num_workers = len(worker_spec)\n",
    "\n",
    "  # construct the cluster and server\n",
    "  cluster = tf.train.ClusterSpec({\"ps\": ps_spec, \"worker\": worker_spec})\n",
    "  server = tf.train.Server(\n",
    "      cluster,\n",
    "      job_name=FLAGS.job_name,\n",
    "      task_index=FLAGS.task_index)\n",
    "\n",
    "  if FLAGS.job_name == \"ps\":\n",
    "    server.join()\n",
    "  elif FLAGS.job_name == \"worker\":\n",
    "\n",
    "    # replica the devices\n",
    "    with tf.device(tf.train.replica_device_setter(\n",
    "        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n",
    "        cluster=cluster)):\n",
    "\n",
    "      # weight(s) and bias(es) of the hidden layer\n",
    "      hid_w = tf.Variable(tf.truncated_normal(\n",
    "        [IMAGE_PIXELS * IMAGE_PIXELS, FLAGS.hidden_units],\n",
    "        stddev=1.0 / IMAGE_PIXELS), name=\"hid_w\")\n",
    "      hid_b = tf.Variable(tf.zeros([FLAGS.hidden_units]), name=\"hid_b\")\n",
    "\n",
    "      # weight(s) and bias(es) of the softmax layer\n",
    "      sm_w = tf.Variable(tf.truncated_normal([FLAGS.hidden_units, 10],\n",
    "        stddev=1.0 / math.sqrt(FLAGS.hidden_units)), name=\"sm_w\")\n",
    "      sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\n",
    "\n",
    "      # inputs for future calculation \n",
    "      x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\n",
    "      y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "      # hidden layer computation logic\n",
    "      hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n",
    "      hid = tf.nn.relu(hid_lin)\n",
    "\n",
    "      # softmax layer computation logic\n",
    "      y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n",
    "      cross_entropy = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "\n",
    "      # global step\n",
    "      global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "      # train step\n",
    "      train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n",
    "          cross_entropy, global_step=global_step)\n",
    "\n",
    "      # accuary calculation\n",
    "      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "      # initialization\n",
    "      init_op = tf.initialize_all_variables()\n",
    "      summary_op = tf.merge_all_summaries()\n",
    "      # TODO: (xiao) restore problem(temporary annotated)\n",
    "      # saver = tf.train.Saver(tf.all_variables(), sharded=True)\n",
    "\n",
    "    # create a training supervisor\n",
    "    sv = tf.train.Supervisor(\n",
    "        is_chief=(FLAGS.task_index == 0),\n",
    "        logdir=FLAGS.log_dir,\n",
    "        init_op=init_op,\n",
    "        summary_op=summary_op,\n",
    "\t# TODO: (xiao) checkpoint cannot be restored here\n",
    "\t# 1. figure out how to use sharded saver\n",
    "\t# 2. use hdfs instead when 0.11 released\n",
    "        saver=None,\n",
    "        global_step=global_step,\n",
    "        save_model_secs=600)\n",
    "\n",
    "    # mnist data(exit the system is download_only is set True)\n",
    "    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "\n",
    "    with sv.managed_session(server.target) as sess:\n",
    "\n",
    "      time_begin = time.time()\n",
    "      local_step = 0\n",
    "      step = 0\n",
    "\n",
    "      while not sv.should_stop():\n",
    "        # Training feed\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(FLAGS.batch_size)\n",
    "        train_feed = {x: batch_xs, y_: batch_ys}\n",
    "\n",
    "        # perform training\n",
    "        _, step = sess.run([train_step, global_step], feed_dict=train_feed)\n",
    "        local_step += 1\n",
    "\n",
    "        now = time.time()\n",
    "        print(\"%f: Worker %d: training step %d done (global step: %d)\"\n",
    "            % (now, FLAGS.task_index, local_step, step))\n",
    "\n",
    "        if step >= FLAGS.training_steps:\n",
    "          break\n",
    "\n",
    "      time_end = time.time()\n",
    "      training_time = time_end - time_begin\n",
    "      print(\"Training elapsed time: %f s\" % training_time)\n",
    "\n",
    "      # Validation feed\n",
    "      val_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "      val_xent = sess.run(cross_entropy, feed_dict=val_feed)\n",
    "      print(\"After %d training step(s), validation cross entropy = %g\"\n",
    "          % (FLAGS.training_steps, val_xent))\n",
    "\n",
    "      # let's calculate the accuracy\n",
    "      pred_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "      print(\"Accuracy is: %f\" % sess.run(accuracy, feed_dict=pred_feed))\n",
    "\n",
    "    # sv.stop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previous Chapter: Recurrent Neural Network](7-rnn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
