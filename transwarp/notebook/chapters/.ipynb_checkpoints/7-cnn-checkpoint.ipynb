{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previous Chapter: AutoEncoder](6-autoencoder.ipynb)\n",
    "<br>\n",
    "[Next Chapter: Recurrent Neural Network](8-rnn.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "In machine learning, a *Convolutional Neural Network* (CNN, or ConvNet) is a type of feed-forward artificial neural network in which the connectivity pattern between its neurons is inspired by the organization of the animal visual cortex. Individual neurons of the animal cortex are arranged in such a way that they respond to overlapping regions tiling the visual field, which can mathematically be described by a convolution operation. Convolutional networks were inspired by biological processes and are variations of multilayer perceptrons designed to use minimal amounts of preprocessing. They have wide applications in **image and video recognition**, **recommender systems** and **natural language processing**.\n",
    "\n",
    "Following note is partially quoted from [Standford Deep Learning Tutorial](http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/).\n",
    "\n",
    "### Note: Convolutional Neural Network(CNN)\n",
    "\n",
    "#### Structure\n",
    "![cnnStructure](http://parse.ele.tue.nl/cluster/2/CNNArchitecture.jpg)\n",
    "\n",
    "#### Convolution\n",
    "Since images have the 'stationary' property, which implies that features that are useful in one region are also likely to be useful for other regions. Thus, we use a rather small patch than the image size to convolve this image. For example, when having an image with m * m * r, we could use a n * n * q patch(where n < m && q <= r). The output will be produced in size m - n + 1.\n",
    "\n",
    "<img src=\"http://ufldl.stanford.edu/tutorial/images/Convolution_schematic.gif\" width=\"440\"/>)\n",
    "\n",
    "#### Pooling\n",
    "To further reduce computation, pooling is introduced in CNN. As mentioned previously, a mean pooling is applied into each region of the image because of the 'stationary' property. Pooling usually ranges from 2 to 5.\n",
    "\n",
    "<img src=\"http://ufldl.stanford.edu/tutorial/images/Pooling_schematic.gif\" width=\"500\"/>)\n",
    "\n",
    "#### Others\n",
    "* Densely Connected Layers: after several convolutional layers, a few densely conncetedly layers are usually constructed before the output layer;\n",
    "* Top Layer Classifier: a top classifier is used to do supervised learning on CNN;\n",
    "* Back Propogation: unsample on pooling layer and use the flipped filter on convolution layer;\n",
    "Here is an exercise of building a *Convolutional Neural Network* using *Tensorflow*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## header\n",
    "###### write your code here ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MNIST data\n",
    "mnist = ... ###### write your code here ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## parameters\n",
    "learning_rate = ... ###### write your code here ######\n",
    "training_iters = ... ###### write your code here ######\n",
    "batch_size = ... ###### write your code here ######\n",
    "display_step = ... ###### write your code here ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## placeholder inputs\n",
    "x = ... ###### write your code here ######\n",
    "y = ... ###### write your code here ######\n",
    "# dropout is a probability for randomly dropping a unit away, it should be a float 32 value\n",
    "dropout = ... ###### write your code here ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## weights and biases\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd': tf.Variable(tf.random_normal([7 * 7 * 64, 1024])),\n",
    "    'output': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd': tf.Variable(tf.random_normal([1024])),\n",
    "    'output': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph\n",
    "\n",
    "In a traditional *Convolutional Neural Network*, we have several *convolution layers* and *pool layers* following by a *fully-connected layer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Covolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convolutional layer\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pool Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max pool layer\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1],\n",
    "                          strides=[1, k, k, 1],\n",
    "                          padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # reshape\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # convolution layer 1\n",
    "    conv1 = ... ###### write your code here ######\n",
    "    # max pooling layer 1\n",
    "    conv1 = ... ###### write your code here ######\n",
    "\n",
    "    # convolution layer 2\n",
    "    conv2 = ... ###### write your code here ######\n",
    "    # max pooling layer 2\n",
    "    conv2 = ... ###### write your code here ######\n",
    "\n",
    "    # fully connected layer\n",
    "    # reshape conv2 output to fit fully connected layer input\n",
    "    fc = tf.reshape(conv2, [-1, weights['wd'].get_shape().as_list()[0]])\n",
    "    # apply `tf.nn.relu()` on linear combination of `fc * w[wd] + b[wd]`\n",
    "    fc = ... ###### write your code here ######\n",
    "    \n",
    "    # apply dropout on fully connected layer\n",
    "    fc = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # output is also a linear combination\n",
    "    output = ... ###### write your code here ######\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicted value\n",
    "pred = conv_net(x, weights, biases, dropout)\n",
    "\n",
    "# cost and optimizer\n",
    "cost = ... ###### write your code here ######\n",
    "optimizer = ... ###### write your code here ######\n",
    "training_steps = ... ###### write your code here ######\n",
    "\n",
    "# accuracy\n",
    "correct_prediction = ... ###### write your code here ######\n",
    "accuracy = ... ###### write your code here ######\n",
    "\n",
    "# initialization\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "\n",
    "    # keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # run training steps, use `x = batch_x`, `y = batch_y` and `dropout = 0.5` here\n",
    "        sess.run(... ###### write your code here ######)\n",
    "\n",
    "        if step % display_step == 0:\n",
    "            # run batch loss and accuracy, use `x = batch_x`, `y = batch_y` and `dropout = 1` here\n",
    "            loss, acc = ... ###### write your code here ######\n",
    "\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256],\n",
    "                                      dropout: 1.}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previous Chapter: AutoEncoder](6-autoencoder.ipynb)\n",
    "<br>\n",
    "[Next Chapter: Recurrent Neural Network](8-rnn.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
